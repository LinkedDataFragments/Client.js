#!/usr/bin/env node
/*! @license MIT Â©2013-2016 Ruben Verborgh, Ghent University - imec */
/* Command-line utility to execute SPARQL queries over triple pattern fragments. */

var ldf = require('../ldf-client');

// Retrieve and check arguments
var args = require('minimist')(process.argv.slice(2));
if (args.listformats)
  return Object.keys(ldf.SparqlResultWriter.writers).forEach(function (t) { console.log(t); });
if (!(args.q || args.f) && args._.length < 2 || args._.length < 1 || args.h || args.help) {
  console.error([
    'The ldf-client tool evaluates a SPARQL query over one of more datasets',
    'that are published as Triple Pattern Fragments.',
    '',
    'Usage:',
    '  ldf-client http://fragments.example.org/dataset [-q] \'SELECT * WHERE { ?s ?p ?o }\'',
    '  ldf-client http://fragments.example.org/dataset [-f] query.sparql',
    '  ldf-client fragment1 fragment2 fragment3 ... [-q] \'SELECT * WHERE { ?s ?p ?o }\'',
    '  ldf-client fragment1 fragment2 fragment3 ... [-f] query.sparql',
    '',
    'Options:',
    '   -q               evaluates the given SPARQL query',
    '   -f               evaluates the SPARQL query in the given file',
    '   -c               use the given JSON configuration file (e.g., config.json)',
    '   -t               determines the MIME type of the output (e.g., application/json)',
    '   -l               sets the log level (e.g., debug, warn, info)',
    '   -d               sets a datetime for querying Memento-enabled archives',
    '   --csv            outputs logging in CSV format (only if logging level is set)',
    '   --help           prints this message',
    '   --listformats    prints the supported MIME types',
  ].join('\n'));
  return process.exit(1);
}

// Load main libraries (postponed to here for speed)
var fs = require('fs'),
    path = require('path'),
    N3 = require('n3'),
    Logger = require('../ldf-client').Logger;

// Parse and initialize configuration
var configFile = args.c ? args.c : path.join(__dirname, '../config-default.json'),
    config = JSON.parse(fs.readFileSync(configFile, { encoding: 'utf8' })),
    queryFile = args.f || args.q || args._.pop(),
    startFragments = args._,
    query = args.q || (args.f || fs.existsSync(queryFile) ? fs.readFileSync(queryFile, 'utf8') : queryFile),
    mimeType = args.t || 'application/json',
    datetime = args.d || config.datetime;

// parse memento datetime
if (datetime)
  config.datetime = datetime === true ? new Date() : new Date(datetime);

// Configure logging
Logger.setLevel(args.l || 'warning');
if (args.csv) Logger.setMode('CSV');

// Execute the query and output its results
config.fragmentsClient = new ldf.FragmentsClient(startFragments, config);
try {
  var sparqlIterator = new ldf.SparqlIterator(query, config), writer;
  switch (sparqlIterator.queryType) {
  // Write JSON representations of the rows or boolean
  case 'ASK':
  case 'SELECT':
    writer = ldf.SparqlResultWriter.instantiate(mimeType, sparqlIterator);
    writer.on('data', function (data) { process.stdout.write(data); });
    break;
  // Write an RDF representation of all results
  case 'CONSTRUCT':
  case 'DESCRIBE':
    config.end = false; // stdout cannot be closed
    writer = new N3.Writer(process.stdout, config);
    sparqlIterator.on('data', function (triple) { writer.addTriple(triple); })
                  .on('end',  function () { writer.end(); });
    break;
  default:
    throw new ldf.SparqlIterator.UnsupportedQueryError(query);
  }

  // Report an error's stack trace
  sparqlIterator.on('error', function (error) {
    console.error('ERROR: An error occurred during query execution.\n');
    console.error(error.stack);
  });
}
// Report a synchronous error
catch (error) {
  console.error('ERROR: Query execution could not start.\n');
  switch (error.name) {
  case 'InvalidQueryError':
  case 'UnsupportedQueryError':
    console.error(error.message);
    break;
  default:
    console.error(error.stack);
  }
}
